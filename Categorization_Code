def load_and_preprocess_data(file_path, column_name):
    df = pd.read_excel(file_path)
    # Basic text preprocessing
    df[column_name] = df[column_name].str.lower().str.strip()
    return df


def load_and_preprocess_data(file_path, column_name):
    df = pd.read_excel(file_path)
    # Basic text preprocessing
    df[column_name] = df[column_name].str.lower().str.strip()
    return df

def load_keyword_rules(excel_path):
    excel_data = pd.ExcelFile(excel_path)
    interventions_df = pd.read_excel(excel_data, sheet_name='Interventions')
    mapping_rules = {}
    for index, row in interventions_df.iterrows():
        area_of_focus = row['Area of Focus']
        mapping_rules[area_of_focus] = {}
        for level in range(1, 6):
            level_key = f"Level {level}"
            keyword_key = f"Level {level} keywords"
            if pd.notna(row[level_key]) and pd.notna(row[keyword_key]):
                mapping_rules[area_of_focus][level_key] = {
                    'description': row[level_key],
                    'keywords': row[keyword_key]
                }
    return mapping_rules

def match_keywords(text, keyword_pattern):
    # Split the pattern by slashes to handle the "two or more" rule
    slash_split = keyword_pattern.split('/')
    if len(slash_split) > 1:
        # We need at least two matches from the split parts
        matches = [any(re.findall(f"\\b{part}\\b", text)) for part in slash_split]
        return sum(matches) >= 2
    else:
        # Handle the parentheses grouping
        paren_split = re.findall(r'\((.*?)\)', keyword_pattern)
        if paren_split:
            return any(re.search(r'\b{}\b'.format(re.escape(word)), text) for word in paren_split)
        else:
            # Direct match without special rules
            return re.search(r'\b{}\b'.format(re.escape(keyword_pattern)), text)

def assign_categories(df, mapping_rules, column_name):
    df['Area of Focus'] = None
    df['Level'] = None
    df['Category'] = None

    for idx, row in df.iterrows():
        text = row[column_name]
        for area, levels in mapping_rules.items():
            for level, details in levels.items():
                if match_keywords(text, details['keywords']):
                    df.at[idx, 'Area of Focus'] = area
                    df.at[idx, 'Level'] = level
                    df.at[idx, 'Category'] = details['description']
                    break
            if df.at[idx, 'Area of Focus'] is not None:
                break

    return df

# Path to your Excel file containing the DataFrame
data_path = 'path_to_your_data.xlsx'

# Load and preprocess your DataFrame
df = load_and_preprocess_data(data_path, 'InterventionDetailsAndObjectivesDescription')

# Load mapping rules from another Excel sheet
rules_path = 'path_to_rules_file.xlsx'
mapping_rules = load_keyword_rules(rules_path)

# Apply the matching and assign categories
df = assign_categories(df, mapping_rules, 'InterventionDetailsAndObjectivesDescription')

# Print the result to check
print(df[['InterventionDetailsAndObjectivesDescription', 'Area of Focus', 'Level', 'Category']])

------------------------------------ Displaying keywords considered for mapping in final dataframe -----------------

def match_keywords(text, keyword_pattern):
    # Split the pattern by slashes to handle the "two or more" rule
    slash_split = keyword_pattern.split('/')
    found_keywords = []
    if len(slash_split) > 1:
        # We need at least two matches from the split parts
        matches = [(part, re.findall(f"\\b{part}\\b", text)) for part in slash_split]
        match_counts = sum([bool(match[1]) for match in matches])
        if match_counts >= 2:
            found_keywords = [match[0] for match in matches if match[1]]
    else:
        # Handle the parentheses grouping
        paren_split = re.findall(r'\((.*?)\)', keyword_pattern)
        if paren_split:
            for word in paren_split:
                if re.search(r'\b{}\b'.format(re.escape(word)), text):
                    found_keywords.append(word)
        else:
            # Direct match without special rules
            if re.search(r'\b{}\b'.format(re.escape(keyword_pattern)), text):
                found_keywords.append(keyword_pattern)

    return found_keywords if found_keywords else None

def assign_categories(df, mapping_rules, column_name):
    df['Area of Focus'] = None
    df['Level'] = None
    df['Category'] = None
    df['Matching Keywords'] = None  # New column to store the matched keywords

    for idx, row in df.iterrows():
        text = row[column_name].lower()  # Ensure the text is in lower case for matching
        for area, levels in mapping_rules.items():
            for level, details in levels.items():
                matching_keywords = match_keywords(text, details['keywords'])
                if matching_keywords:
                    df.at[idx, 'Area of Focus'] = area
                    df.at[idx, 'Level'] = level
                    df.at[idx, 'Category'] = details['description']
                    df.at[idx, 'Matching Keywords'] = ", ".join(matching_keywords)  # Join list of keywords into a string
                    break
            if df.at[idx, 'Area of Focus'] is not None:
                break

    return df
---------------------------------------- Using multiple columns for mapping --------------------------------

def concatenate_columns(df, columns):
    # Creating a new column 'ConcatenatedText' that merges the text from the specified columns
    df['ConcatenatedText'] = df[columns].apply(lambda x: ' '.join(x.dropna().astype(str)).lower(), axis=1)
    return df

def match_keywords(text, keyword_pattern):
    slash_split = keyword_pattern.split('/')
    found_keywords = []
    if len(slash_split) > 1:
        matches = [(part, re.findall(f"\\b{part}\\b", text)) for part in slash_split]
        match_counts = sum([bool(match[1]) for match in matches])
        if match_counts >= 2:
            found_keywords = [match[0] for match in matches if match[1]]
    else:
        paren_split = re.findall(r'\((.*?)\)', keyword_pattern)
        if paren_split:
            for word in paren_split:
                if re.search(r'\b{}\b'.format(re.escape(word)), text):
                    found_keywords.append(word)
        else:
            if re.search(r'\b{}\b'.format(re.escape(keyword_pattern)), text):
                found_keywords.append(keyword_pattern)

    return found_keywords if found_keywords else None

def assign_categories(df, mapping_rules, text_column='ConcatenatedText'):
    df['Area of Focus'] = None
    df['Level'] = None
    df['Category'] = None
    df['Matching Keywords'] = None

    for idx, row in df.iterrows():
        text = row[text_column]  # Use the concatenated text for keyword matching
        for area, levels in mapping_rules.items():
            for level, details in levels.items():
                matching_keywords = match_keywords(text, details['keywords'])
                if matching_keywords:
                    df.at[idx, 'Area of Focus'] = area
                    df.at[idx, 'Level'] = level
                    df.at[idx, 'Category'] = details['description']
                    df.at[idx, 'Matching Keywords'] = ", ".join(matching_keywords)
                    break
            if df.at[idx, 'Area of Focus'] is not None:
                break

    return df

# Assuming you have already loaded your DataFrame `df`
columns_to_use = ['InterventionTypeDescription', 'InterventionDetailsAndObjectivesDescription']

# Concatenate the specified columns into a single column for processing
df = concatenate_columns(df, columns_to_use)

# Load the keyword mapping rules (assuming this has been already defined)
mapping_rules = load_keyword_rules('path_to_your_rules_file.xlsx')

# Apply the keyword matching and category assignment
df = assign_categories(df, mapping_rules)

# Print the result to verify the output
print(df[['ConcatenatedText', 'Area of Focus', 'Level', 'Category', 'Matching Keywords']])

################# Bert tokenizor integration #########################

from transformers import BertTokenizer, BertModel
import torch
import pandas as pd

# Load tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

def add_bert_features(df, text_column):
    # This function assumes that CUDA is available. If not, remove '.to("cuda")' below
    model.to("cuda")
    
    # Function to get BERT features for a single text
    def get_bert_features(text):
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        # Move tensors to the same device as the model
        inputs = {key: value.to("cuda") for key, value in inputs.items()}
        with torch.no_grad():  # Ensure no gradients are calculated
            outputs = model(**inputs)
        # Retrieve the last hidden states and move to CPU
        features = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Get the embeddings of the [CLS] token
        return features

    # Apply the function to each item in the column and store the results as a new column
    df['BERT_Features'] = df[text_column].apply(lambda x: get_bert_features(x))

    return df

# Assuming you have already loaded your DataFrame `df` and concatenated the columns
# Now add BERT features
df = add_bert_features(df, 'ConcatenatedText')

# Print the result to verify the output
print(df[['ConcatenatedText', 'BERT_Features']])

############## Using Spacy for cleaning and Bert for embedding #############

# Load spaCy and BERT

import spacy
from transformers import BertTokenizer, BertModel
import torch
import pandas as pd

# Load spaCy NLP model
nlp = spacy.load('en_core_web_sm')

# Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()  # Set the model to evaluation mode if not training

# Define Preprocessing and Feature Extraction Functions

def preprocess_text(text):
    """ Use spaCy to clean and lemmatize text. """
    doc = nlp(text)
    cleaned_text = ' '.join(token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.text not in ['nonrelevant'])
    return cleaned_text

def get_bert_features(text):
    """ Extract BERT features for a single text input. """
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    features = outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # Get the embeddings of the [CLS] token
    return features

def add_features_to_df(df, text_column):
    """ Preprocess text and then add BERT features to the DataFrame. """
    df['CleanedText'] = df[text_column].apply(preprocess_text)
    df['BERT_Features'] = df['CleanedText'].apply(get_bert_features)
    return df

# Apply the Integrated Processing to DataFrame

# Assuming 'ConcatenatedText' is already in your DataFrame
df = add_features_to_df(df, 'ConcatenatedText')

# Print the result to verify outputs
print(df[['ConcatenatedText', 'CleanedText', 'BERT_Features']])





############## machine learning model to predict categories with bert embeddings ###########


import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

def train_and_predict_with_bert_features(df, feature_column, target_column):
    # Filter out rows where the category is not yet mapped
    mapped_df = df.dropna(subset=[target_column, feature_column])

    # Prepare the features and labels
    X = np.stack(mapped_df[feature_column].values)  # Assuming BERT features are already in a suitable numpy array format
    y = mapped_df[target_column].values

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize and train the classifier
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)

    # Predict on the test set
    y_pred = clf.predict(X_test)

    # Evaluate the model
    print("Accuracy on test set:", accuracy_score(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

    # Predicting the unmapped categories in the original DataFrame
    unmapped_index = df[df[target_column].isnull()].index
    unmapped_features = np.stack(df.loc[unmapped_index, feature_column].values)
    unmapped_predictions = clf.predict(unmapped_features)
    df.loc[unmapped_index, target_column] = unmapped_predictions

    return df


# Assume 'df' is your DataFrame and 'BERT_Features' are stored from previous steps
# 'Category' is the target column where some entries are already mapped

df = train_and_predict_with_bert_features(df, 'BERT_Features', 'Category')

# Now df has the previously unmapped categories filled in
print(df[['ConcatenatedText', 'Category']])

##################################################################





