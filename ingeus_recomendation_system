import pandas as pd
from sklearn.metrics import precision_score, recall_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler
from sklearn.impute import SimpleImputer

# Assume the data preparation steps have already been performed, resulting in train_pdf and test_pdf
# and features_to_include list is defined with necessary features.

# Preprocessor as defined in your original pipeline
BOOL_COLS = [i for i in features_to_include if data_types[i] == "Boolean"]
NUMERICAL_COLS = [i for i in features_to_include if data_types[i] == "Numerical"]
CATEGORICAL_COLS = [i for i in features_to_include if data_types[i] == "Categorical"]

bool_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', RobustScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('bool', bool_transformer, BOOL_COLS),
    ('numerical', numerical_transformer, NUMERICAL_COLS),
    ('categorical', categorical_transformer, CATEGORICAL_COLS)
])

# Apply the preprocessor to the training data
X_train = train_pdf[features_to_include]
X_test = test_pdf[features_to_include]

# Fit and transform the training data
X_train_transformed = preprocessor.fit_transform(X_train)

# Transform the test data
X_test_transformed = preprocessor.transform(X_test)

# Fit Nearest Neighbors model on the transformed training data
nn = NearestNeighbors(n_neighbors=5, metric='cosine')
nn.fit(X_train_transformed)

# Function to find similar participants
def find_similar_participants(new_participant_data, X_train, n_neighbors=5):
    distances, indices = nn.kneighbors(new_participant_data.reshape(1, -1), n_neighbors=n_neighbors)
    return train_pdf.iloc[indices[0]]

# Function to evaluate the recommendations
def evaluate_recommendations(recommended_interventions, actual_interventions, top_n=10):
    # Convert to binary format for precision and recall
    y_true = [1 if intervention in actual_interventions else 0 for intervention in recommended_interventions]
    y_pred = [1] * top_n  # All recommended interventions are considered as positive

    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    
    return {
        'Precision': precision,
        'Recall': recall
    }

# Generate recommendations for each participant in the test set
all_recommendations = []
all_actuals = []

for index, row in X_test.iterrows():
    new_participant_data = X_test_transformed[index]
    similar_participants = find_similar_participants(new_participant_data, X_train_transformed)
    successful_participants = similar_participants[similar_participants['job_out_come'] == 1]
    mean_feature_values = successful_participants[features_to_include].mean()
    ranked_features = mean_feature_values.sort_values(ascending=False)
    recommended_interventions = [feature for feature in ranked_features.index if 'areaoffocus_' in feature or 'level_' in feature or 'category_']
    recommendations = recommended_interventions[:10]

    all_recommendations.append(recommendations)
    actual_interventions = row[features_to_include].index[row[features_to_include] > 0].tolist()
    all_actuals.append(actual_interventions)

# Evaluate recommendations
precision_scores = []
recall_scores = []

for recs, actuals in zip(all_recommendations, all_actuals):
    results = evaluate_recommendations(recs, actuals)
    precision_scores.append(results['Precision'])
    recall_scores.append(results['Recall'])

average_precision = np.mean(precision_scores)
average_recall = np.mean(recall_scores)

print(f'Average Precision: {average_precision}')
print(f'Average Recall: {average_recall}')

######################

IndexError                                Traceback (most recent call last)
Cell In[101], line 253
    250 all_actuals = []
    252 for index, row in X_test.iterrows():
--> 253     new_participant_data = X_test_transformed[index]
    254     similar_participants = find_similar_participants(new_participant_data, X_train_transformed)
    255     successful_participants = similar_participants[similar_participants['job_out_come'] == 1]

File ~/cluster-env/trident_env/lib/python3.10/site-packages/scipy/sparse/_index.py:47, in IndexMixin.__getitem__(self, key)
     46 def __getitem__(self, key):
---> 47     row, col = self._validate_indices(key)
     49     # Dispatch to specialized methods.
     50     if isinstance(row, INT_TYPES):

File ~/cluster-env/trident_env/lib/python3.10/site-packages/scipy/sparse/_index.py:155, in IndexMixin._validate_indices(self, key)
    153 row = int(row)
    154 if row < -M or row >= M:
--> 155     raise IndexError('row index (%d) out of range' % row)
    156 if row < 0:
    157     row += M

IndexError: row index (35311) out of range

############################

import pandas as pd
from sklearn.metrics import precision_score, recall_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler
from sklearn.impute import SimpleImputer
import numpy as np

# Assume the data preparation steps have already been performed, resulting in train_pdf and test_pdf
# and features_to_include list is defined with necessary features.

# Preprocessor as defined in your original pipeline
BOOL_COLS = [i for i in features_to_include if data_types[i] == "Boolean"]
NUMERICAL_COLS = [i for i in features_to_include if data_types[i] == "Numerical"]
CATEGORICAL_COLS = [i for i in features_to_include if data_types[i] == "Categorical"]

bool_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', RobustScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('bool', bool_transformer, BOOL_COLS),
    ('numerical', numerical_transformer, NUMERICAL_COLS),
    ('categorical', categorical_transformer, CATEGORICAL_COLS)
])

# Apply the preprocessor to the training data
X_train = train_pdf[features_to_include]
X_test = test_pdf[features_to_include]

# Fit and transform the training data
X_train_transformed = preprocessor.fit_transform(X_train)

# Transform the test data
X_test_transformed = preprocessor.transform(X_test)

# Fit Nearest Neighbors model on the transformed training data
nn = NearestNeighbors(n_neighbors=5, metric='cosine')
nn.fit(X_train_transformed)

# Function to find similar participants
def find_similar_participants(new_participant_data, n_neighbors=5):
    distances, indices = nn.kneighbors(new_participant_data.reshape(1, -1), n_neighbors=n_neighbors)
    return train_pdf.iloc[indices[0]]

# Function to evaluate the recommendations
def evaluate_recommendations(recommended_interventions, actual_interventions, top_n=10):
    # Convert to binary format for precision and recall
    y_true = [1 if intervention in actual_interventions else 0 for intervention in recommended_interventions]
    y_pred = [1] * top_n  # All recommended interventions are considered as positive

    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    
    return {
        'Precision': precision,
        'Recall': recall
    }

# Generate recommendations for each participant in the test set
all_recommendations = []
all_actuals = []

for index in range(X_test_transformed.shape[0]):
    new_participant_data = X_test_transformed[index]
    similar_participants = find_similar_participants(new_participant_data)
    successful_participants = similar_participants[similar_participants['job_out_come'] == 1]
    mean_feature_values = successful_participants[features_to_include].mean()
    ranked_features = mean_feature_values.sort_values(ascending=False)
    recommended_interventions = [feature for feature in ranked_features.index if 'areaoffocus_' in feature or 'level_' in feature or 'category_']
    recommendations = recommended_interventions[:10]

    all_recommendations.append(recommendations)
    actual_interventions = X_test.iloc[index][features_to_include].index[X_test.iloc[index][features_to_include] > 0].tolist()
    all_actuals.append(actual_interventions)

# Evaluate recommendations
precision_scores = []
recall_scores = []

for recs, actuals in zip(all_recommendations, all_actuals):
    results = evaluate_recommendations(recs, actuals)
    precision_scores.append(results['Precision'])
    recall_scores.append(results['Recall'])

average_precision = np.mean(precision_scores)
average_recall = np.mean(recall_scores)

print(f'Average Precision: {average_precision}')
print(f'Average Recall: {average_recall}')

################################################

2024-07-25:00:36:00,218 WARNING  [tracking_store.py:153] log_inputs not supported
2024-07-25:00:36:04,383 WARNING  [tracking_store.py:153] log_inputs not supported
2024-07-25:00:36:07,758 WARNING  [tracking_store.py:153] log_inputs not supported
2024-07-25:00:36:08,3 WARNING  [tracking_store.py:153] log_inputs not supported
2024-07-25:00:36:08,131 WARNING  [tracking_store.py:153] log_inputs not supported
2024-07-25:00:36:09,246 WARNING  [tracking_store.py:153] log_inputs not supported
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[104], line 86
     83     recommendations = recommended_interventions[:10]
     85     all_recommendations.append(recommendations)
---> 86     actual_interventions = X_test.iloc[index][features_to_include].index[X_test.iloc[index][features_to_include] > 0].tolist()
     87     all_actuals.append(actual_interventions)
     89 # Evaluate recommendations

File ~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/ops/common.py:81, in _unpack_zerodim_and_defer.<locals>.new_method(self, other)
     77             return NotImplemented
     79 other = item_from_zerodim(other)
---> 81 return method(self, other)

File ~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/arraylike.py:56, in OpsMixin.__gt__(self, other)
     54 @unpack_zerodim_and_defer("__gt__")
     55 def __gt__(self, other):
---> 56     return self._cmp_method(other, operator.gt)

File ~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/series.py:6096, in Series._cmp_method(self, other, op)
   6093 rvalues = extract_array(other, extract_numpy=True, extract_range=True)
   6095 with np.errstate(all="ignore"):
-> 6096     res_values = ops.comparison_op(lvalues, rvalues, op)
   6098 return self._construct_result(res_values, name=res_name)

File ~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:293, in comparison_op(left, right, op)
    290     return invalid_comparison(lvalues, rvalues, op)
    292 elif is_object_dtype(lvalues.dtype) or isinstance(rvalues, str):
--> 293     res_values = comp_method_OBJECT_ARRAY(op, lvalues, rvalues)
    295 else:
    296     res_values = _na_arithmetic_op(lvalues, rvalues, op, is_cmp=True)

File ~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:82, in comp_method_OBJECT_ARRAY(op, x, y)
     80     result = libops.vec_compare(x.ravel(), y.ravel(), op)
     81 else:
---> 82     result = libops.scalar_compare(x.ravel(), y, op)
     83 return result.reshape(x.shape)

File ~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/_libs/ops.pyx:107, in pandas._libs.ops.scalar_compare()

TypeError: '>' not supported between instances of 'str' and 'int'

