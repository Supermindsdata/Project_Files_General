%pip install scikit-learn==1.5.0 xgboost==1.7.2

from pyspark.sql import DataFrame
from pyspark.sql.functions import abs, mean, col, lit, when
from decimal import Decimal
from pyspark.sql.functions import col, datediff, to_date
from pyspark.sql import functions as F
from pyspark.sql.functions import col
from pyspark.sql import functions as psf


import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_curve, auc
from joblib import dump
import seaborn as sns
import matplotlib.pyplot as plt

import mlflow
from mlflow.models import infer_signature

############### Functions ###################

def get_feature_importances(model, features_to_include):

    """
    Calculates and aggregates feature importances from a given model for specified features.

    Args:
    - model (Pipeline): A scikit-learn Pipeline object containing a trained model.
    - features_to_include (list): A list of strings representing feature names to include in the feature importances calculation.

    Returns:
    - pandas.DataFrame: A DataFrame containing aggregated feature importances for the specified features.
    """

    # Get feature importance
    feature_importance = model.named_steps['classifier'].feature_importances_

    # Get the transformed feature names
    transformed_feature_names = model.named_steps['preprocessor'].get_feature_names_out()

    # Create a dictionary to map the transformed feature names to the original feature names
    feature_map = {}
    for transformed_feature in transformed_feature_names:
        for feature in features_to_include:
            if feature in transformed_feature:
                if feature not in feature_map:
                    feature_map[feature] = []
                feature_map[feature].append(transformed_feature)
                break

    # Calculate the aggregated feature importance for each original feature
    aggregated_importance = {}
    for original_feature, transformed_features in feature_map.items():
        importance_sum = sum(feature_importance[transformed_feature_names.tolist().index(feature)] for feature in transformed_features)
        aggregated_importance[original_feature] = importance_sum

    # Create a DataFrame of feature importances
    importance_df = pd.DataFrame(list(aggregated_importance.items()), columns=['Feature', 'Importance'])
    importance_df.sort_values('Importance', ascending=False, inplace=True)
    importance_df.reset_index(drop=True, inplace=True)

    return importance_df


################### Get Data #######################

################# Config ########################

features_to_include

data_types ={
'MarketingEmailOpen_LastWeek': 'Numerical',
'InactivityPaymentFailureReason': 'Categorical',
'InactivityAutoRenewStatus': 'Boolean'
}

fill_with_zero_columns

classifiers = [
    {
        'name': 'Gradient Boosting',
        'classifier': GradientBoostingClassifier(),
        'param_grid': {
            'classifier__n_estimators': [100, 200],
            'classifier__max_depth': [3, 5],
            'classifier__learning_rate': [0.1, 0.01]
        }
    },
    {
        'name': 'Random Forest',
        'classifier': RandomForestClassifier(),
        'param_grid': {
            'classifier__n_estimators': [100, 200],
            'classifier__max_depth': [None, 5, 10]
        }
    },
    {
        'name': 'XGBoost',
        'classifier': XGBClassifier(),
        'param_grid': {
            'classifier__n_estimators': [100, 200],
            'classifier__max_depth': [3, 5],
            'classifier__learning_rate': [0.1, 0.01]
        }
    },
    {
        'name': 'LightGBM',
        'classifier': LGBMClassifier(),
        'param_grid': {
            'classifier__n_estimators': [100, 200],
            'classifier__max_depth': [3, 5],
            'classifier__learning_rate': [0.1, 0.01]
        }
    },
]

BOOL_COLS = [i for i in features_to_include if data_types[i] == "Boolean"]

NUMERICAL_COLS = [i for i in features_to_include if data_types[i] == "Numerical"]

CATEGORICAL_COLS = [
    i for i in features_to_include if data_types[i] == "Categorical"
]

######## Fill Missing Values ##########

sdf_filled = sdf.fillna(0, subset=fill_with_zero_columns)

########## Test Classifiers and Params #####

# Since our data is already balanced, we will convert it pandas dataframe and take a stratified test sample
pdf = sdf_filled.toPandas()

# Split the data into train and test sets with stratification
train_pdf, test_pdf = train_test_split(pdf, test_size=0.3, stratify=pdf['churn_flag'], random_state=42)

######## Training Pipeline #########

# Preprocess boolean columns
for col_name in BOOL_COLS:
    train_pdf[col_name] = train_pdf[col_name].astype(str)
    test_pdf[col_name] = test_pdf[col_name].astype(str)

# Preprocess numerical columns
for col_name in NUMERICAL_COLS:
    train_pdf[col_name] = pd.to_numeric(train_pdf[col_name], errors='coerce')
    test_pdf[col_name] = pd.to_numeric(test_pdf[col_name], errors='coerce')

# Preprocess categorical columns
for col_name in CATEGORICAL_COLS:
    train_pdf[col_name] = train_pdf[col_name].fillna('')
    test_pdf[col_name] = test_pdf[col_name].fillna('')

# Create column transformers
bool_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', RobustScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('bool', bool_transformer, BOOL_COLS),
    ('numerical', numerical_transformer, NUMERICAL_COLS),
    ('categorical', categorical_transformer, CATEGORICAL_COLS)
])

# Create an empty DataFrame to store the results
results_df = pd.DataFrame(columns=['Classifier', 'Best Parameters', 'Test AUC', 'Test Precision', 'Test Recall', 'Test F1'])

# Iterate over the classifiers
for cls in classifiers:
    print(f"Training {cls['name']}...")
    
    # Create the pipeline
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', cls['classifier'])])
    
    # Create the grid search object
    grid_search = GridSearchCV(pipeline, cls['param_grid'], cv=3, scoring='roc_auc')
    
    # Balance the training data
    # train_pdf_balanced = pd.concat([
    #     train_pdf[train_pdf['churn_flag'] == 0].sample(n=len(train_pdf[train_pdf['churn_flag'] == 1]), random_state=42),
    #     train_pdf[train_pdf['churn_flag'] == 1]
    # ])
    
    # Split the balanced training data into features and target
    # Since the train data is already balanced we will copy same dataframe
    train_pdf_balanced = train_pdf

    X_train = train_pdf_balanced.drop('churn_flag', axis=1)
    y_train = train_pdf_balanced['churn_flag']
    
    # Fit the grid search object
    grid_search.fit(X_train, y_train)
    
    # Get the best model
    best_model = grid_search.best_estimator_

    # # Balance the test data
    # test_pdf_balanced = pd.concat([
    #     test_pdf[test_pdf['churn_flag'] == 0].sample(n=len(test_pdf[test_pdf['churn_flag'] == 1]), random_state=42),
    #     test_pdf[test_pdf['churn_flag'] == 1]
    # ])
    
    # Split the balanced test data into features and target
    # Since the test data is already balanced we will copy same dataframe
    test_pdf_balanced = test_pdf

    X_test = test_pdf_balanced.drop('churn_flag', axis=1)
    y_test = test_pdf_balanced['churn_flag']
    
    # Make predictions on the training data
    y_train_pred = best_model.predict(X_train)
    y_train_pred_proba = best_model.predict_proba(X_train)[:, 1]
    
    # Compute metrics for the training data
    train_auc = roc_auc_score(y_train, y_train_pred_proba)
    train_precision = precision_score(y_train, y_train_pred)
    train_recall = recall_score(y_train, y_train_pred)
    train_f1 = f1_score(y_train, y_train_pred)
    
    # Make predictions on the test data
    y_pred = best_model.predict(X_test)
    y_pred_proba = best_model.predict_proba(X_test)[:, 1]
    
    # Compute metrics for the test data
    test_auc = roc_auc_score(y_test, y_pred_proba)
    test_precision = precision_score(y_test, y_pred)
    test_recall = recall_score(y_test, y_pred)
    test_f1 = f1_score(y_test, y_pred)
    
    # Store the results in the DataFrame
    results_df = results_df.append({
        'Classifier': cls['name'],
        'Best Parameters': grid_search.best_params_,
        'Train AUC': train_auc,
        'Train Precision': train_precision,
        'Train Recall': train_recall,
        'Train F1': train_f1,
        'Test AUC': test_auc,
        'Test Precision': test_precision,
        'Test Recall': test_recall,
        'Test F1': test_f1
    }, ignore_index=True)
    
       
    # Print the results
    print(f"Best parameters for {cls['name']}: {grid_search.best_params_}")
    print(f"Train AUC for {cls['name']}: {train_auc:.4f}")
    print(f"Train Precision for {cls['name']}: {train_precision:.4f}")
    print(f"Train Recall for {cls['name']}: {train_recall:.4f}")
    print(f"Train F1 for {cls['name']}: {train_f1:.4f}")
    print(f"Test AUC for {cls['name']}: {test_auc:.4f}")
    print(f"Test Precision for {cls['name']}: {test_precision:.4f}")
    print(f"Test Recall for {cls['name']}: {test_recall:.4f}")
    print(f"Test F1 for {cls['name']}: {test_f1:.4f}")
    print("---")

# Display the results DataFrame
#print("Results:")
#results_df.display()

###### Feature Selection for Best Model ############

# Best parameters for Gradient Boosting
params = {
    'classifier__n_estimators': [200],
    'classifier__max_depth': [3],
    'classifier__learning_rate': [0.1]
}

############# ROC Threshold ###############
# Preprocess boolean columns
for col_name in BOOL_COLS:
    train_pdf[col_name] = train_pdf[col_name].astype(str)
    test_pdf[col_name] = test_pdf[col_name].astype(str)

# Preprocess numerical columns
for col_name in NUMERICAL_COLS:
    train_pdf[col_name] = pd.to_numeric(train_pdf[col_name], errors='coerce')
    test_pdf[col_name] = pd.to_numeric(test_pdf[col_name], errors='coerce')

# Preprocess categorical columns
for col_name in CATEGORICAL_COLS:
    train_pdf[col_name] = train_pdf[col_name].fillna('')
    test_pdf[col_name] = test_pdf[col_name].fillna('')

# Create column transformers
bool_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', RobustScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('bool', bool_transformer, BOOL_COLS),
    ('numerical', numerical_transformer, NUMERICAL_COLS),
    ('categorical', categorical_transformer, CATEGORICAL_COLS)
])

# Create the pipeline
classifier = GradientBoostingClassifier(random_state=42)
pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)])
grid_search = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='roc_auc')

# # Balance the training data
train_pdf_balanced = train_pdf
# train_pdf_balanced = pd.concat([
#     train_pdf[train_pdf['churn_flag'] == 0].sample(n=len(train_pdf[train_pdf['churn_flag'] == 1]), random_state=42),
#     train_pdf[train_pdf['churn_flag'] == 1]
# ])

test_pdf_balanced = test_pdf
# test_pdf_balanced = pd.concat([
#     test_pdf[test_pdf['churn_flag'] == 0].sample(n=len(test_pdf[test_pdf['churn_flag'] == 1]), random_state=42),
#     test_pdf[test_pdf['churn_flag'] == 1]
# ])

# Split the balanced training data into features and target
X_train = train_pdf_balanced[features_to_include]
y_train = train_pdf_balanced['churn_flag']

# Fit the pipeline
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

# Assuming get_feature_importances is a function defined elsewhere
feature_importances_df = get_feature_importances(best_model, features_to_include)

# Sort the features by importance in ascending order - start with top 30 features
sorted_features = feature_importances_df.sort_values('Importance')['Feature'].tolist()[-30:]

# Set the ROC AUC threshold - stop removing features when roc goes below threshold
#roc_auc_threshold = 0.695

# Check with new threshold of 0.68
roc_auc_threshold = 0.68

# Create an empty list to store the results
results = []

# # Iterate over the sorted features
# for i in range(len(sorted_features)):
#     # Remove the least important feature
#     selected_features = sorted_features[i+1:]
"""The issue here is that when i is 0, sorted_features[:i] will be an empty list, and sorted_features[i+1:] will be the full list minus the first element. However, as the loop continues, you will eventually remove all features, which is not the intended behavior."""

# Iterate over the sorted features, but always keep at least one feature
for i in range(len(sorted_features) - 1):
    # Remove the least important feature
    selected_features = sorted_features[:-i-1]

    # Select the columns in X_train based on the selected features
    X_train_selected = X_train[selected_features]

    # Create new boolean, numerical, and categorical column lists based on the selected features
    selected_bool_cols = [col for col in BOOL_COLS if col in selected_features]
    selected_numerical_cols = [col for col in NUMERICAL_COLS if col in selected_features]
    selected_categorical_cols = [col for col in CATEGORICAL_COLS if col in selected_features]

    # Create a new preprocessor based on the selected features
    selected_preprocessor = ColumnTransformer(transformers=[
        ('bool', bool_transformer, selected_bool_cols),
        ('numerical', numerical_transformer, selected_numerical_cols),
        ('categorical', categorical_transformer, selected_categorical_cols)
    ])

    # Create a new pipeline with the selected preprocessor and classifier
    selected_pipeline = Pipeline(steps=[
        ('preprocessor', selected_preprocessor),
        ('classifier', classifier)
    ])

    grid_search = GridSearchCV(selected_pipeline, param_grid=params, cv=3, scoring='roc_auc')
    grid_search.fit(X_train_selected, y_train)

    mean_roc_auc = grid_search.cv_results_['mean_test_score'][0]

    best_model = grid_search.best_estimator_

    # Make predictions on the balanced test data
    X_test = test_pdf_balanced.drop('churn_flag', axis=1)
    X_test_selected = X_test[selected_features]
    y_test = test_pdf_balanced['churn_flag']
    y_pred_proba = best_model.predict_proba(X_test_selected)[:, 1]

    # Calculate the ROC AUC
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    res_dict = {
        'Number of Features': len(selected_features),
        'Mean Val ROC AUC': mean_roc_auc,
        'Test ROC AUC': roc_auc,
    }

    # Store the results
    results.append(res_dict)

    print(res_dict)

    # Check if the ROC AUC drops below the threshold
    if roc_auc < roc_auc_threshold:
        break

# Create a DataFrame with the results
results_df = pd.DataFrame(results)

# Display the results DataFrame
print("Results:")
print(results_df)

results_df.display()

######## Print Features ###########

features_selected = feature_importances_df.sort_values('Importance')['Feature'].tolist()[-15:]
print(features_selected)

######### Final Features ###########

final_features = ['PlanTextUKCount_Last6Month', 'PlanDataUKGB_Last6Month', 'TotalPaymentRetryAttempt_Last6Month', 
 'PlanDataUKGB_LastWeek', 'Minute_Month_6Month_Ratio', 'Data_Month_6Month_Ratio', 'AgeRange', 
 'LastRenewalNumber', 'PlanVoiceUKCount_Last6Month', 'PlanVoiceUKMinute_LastWeek', 'Days_Since_LastRenewalDate', 'Channel',#'LeadGeneratorGroup', 
 'Data_Week_Month_Ratio', 'Minute_Week_Month_Ratio']

######### Train Best Model ########

X_train_selected = X_train[final_features]

# Create new boolean, numerical, and categorical column lists based on the selected features
selected_bool_cols = [col for col in BOOL_COLS if col in final_features]
selected_numerical_cols = [col for col in NUMERICAL_COLS if col in final_features]
selected_categorical_cols = [col for col in CATEGORICAL_COLS if col in final_features]

# Create a new preprocessor based on the selected features
selected_preprocessor = ColumnTransformer(transformers=[
    ('bool', bool_transformer, selected_bool_cols),
    ('numerical', numerical_transformer, selected_numerical_cols),
    ('categorical', categorical_transformer, selected_categorical_cols)
])


#classifier = GradientBoostingClassifier(random_state=42,learning_rate=0.1,max_depth=3,n_estimators=200)

classifier = XGBClassifier(random_state=42, learning_rate=0.1, max_depth=3, n_estimators =200)
# Create a new pipeline with the selected preprocessor and classifier
selected_pipeline = Pipeline(steps=[
    ('preprocessor', selected_preprocessor),
    ('classifier', classifier)
])

# Fit the new pipeline
selected_pipeline.fit(X_train_selected, y_train)

# Make predictions on the balanced test data
X_test = test_pdf_balanced.drop('churn_flag', axis=1)
X_test_selected = X_test[final_features]
y_test = test_pdf_balanced['churn_flag']
y_pred = selected_pipeline.predict(X_test_selected)
y_pred_proba = selected_pipeline.predict_proba(X_test_selected)[:, 1]

# Compute metrics
test_auc = roc_auc_score(y_test, y_pred_proba)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
    
res_dict = {
  'Test AUC': test_auc,
  'Test Precision': precision,
  'Test Recall': recall,
  'Test F1': f1
}

print(res_dict)

######### Save Model ############
############## Get ROC Curve ########
# Make predictions on the test set
y_pred_proba = selected_pipeline.predict_proba(X_test_selected)[:, 1]

# Calculate the false positive rate (FPR), true positive rate (TPR), and thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Calculate the area under the ROC curve (AUC)
roc_auc = auc(fpr, tpr)

# Set the plot style and color palette
plt.style.use('default')
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

######### View Feature Importance ###########

df_importances = get_feature_importances(selected_pipeline, final_features)

# Sort the DataFrame by importance in descending order
df_importances = df_importances.sort_values('Importance', ascending=False)

# Set the seaborn style and color palette
sns.set_style('whitegrid')
sns.set_palette('pastel')

# Create a bar plot using seaborn
plt.figure(figsize=(12, 8))
sns.barplot(data=df_importances, x='Importance', y='Feature')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances')
plt.show()

######### Investigating Top Features ########

sdf.filter(psf.col('Minute_Week_Month_Ratio').isNull()).groupby('Minute_Week_Month_Ratio').count().display()
sdf.filter(psf.col('Minute_Week_Month_Ratio').isNotNull()).groupby('Minute_Week_Month_Ratio').count().display()

# Filter out rows where 'LeadGeneratorGroup' is not null
filtered_sdf = sdf.filter(psf.col('LeadGeneratorGroup').isNotNull())

# Group by 'churn_flag' and 'LeadGeneratorGroup', then count the number of occurrences
grouped_counts_sdf = filtered_sdf.groupby('churn_flag', 'LeadGeneratorGroup').count()

# Display the result
grouped_counts_sdf.display()

sdf.filter(psf.col('LeadGeneratorGroup').isNull()).groupby('churn_flag').count().display()
sdf.filter(psf.col('LeadGeneratorGroup').isNotNull()).groupby('churn_flag').count().display()

### Test Set ####
# Filter out rows where 'NextRenewalPlan' is not null
filtered_pdf = test_pdf[test_pdf['Channel'].notnull()]

# Group by 'churn_flag' and 'NextRenewalPlan', then count the number of occurrences
grouped_counts_pdf = filtered_pdf.groupby(['churn_flag', 'Channel']).size().reset_index(name='count')

# Display the result
print(grouped_counts_pdf)

####### Check Correlation #######

# Filter the DataFrame to include only the selected features
df_selected = train_pdf[final_features]

# Filter out non-numerical features based on the data_types dictionary
numerical_features = [feat for feat in final_features if data_types[feat] == "Numerical"]

# Calculate the correlation matrix
corr_matrix = df_selected[numerical_features].corr()

# Create a heatmap using Seaborn
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", linewidths=0.5, square=True,
            annot_kws={"size": 8})  # Adjust the size here as needed
plt.title("Correlation Heatmap of Numerical Features")
plt.tight_layout()
plt.show()

######## View Distribution Score ##########

## Check Buckets ###

# Make predictions on the test set
X_test = test_pdf[final_features]
y_test = test_pdf['churn_flag']
y_pred_proba = selected_pipeline.predict_proba(X_test)[:, 1]

# Create score buckets
score_buckets = np.arange(0, 1.1, 0.1)

# Create a DataFrame with the predicted probabilities
df_proba = pd.DataFrame({'Probability': y_pred_proba})

# Calculate the count of instances for each score bucket
df_count = df_proba.groupby(pd.cut(df_proba['Probability'], score_buckets)).size().reset_index(name='Count')
df_count.columns = ['Score Bucket', 'Count']

# Set the seaborn style and color palette
sns.set_style('whitegrid')
sns.set_palette('pastel')

# Create a bar plot using seaborn
plt.figure(figsize=(10, 6))
sns.barplot(data=df_count, x='Score Bucket', y='Count')
plt.xlabel('Score Bucket')
plt.ylabel('Count')
plt.title('Count of Instances per Score Bucket')
plt.xticks(rotation=45)
plt.show()

########### Examine Top Features #######

churn_percentage = test_pdf.groupby('AgeRange')['churn_flag'].mean() * 100

plt.figure(figsize=(12, 8))
sns.barplot(x=churn_percentage.index, y=churn_percentage)
plt.xlabel('AgeRange')
plt.xticks(rotation=90)  # Rotate x-axis labels to be vertical
plt.show()

##############################
test_pdf['Days_Since_LastRenewalDate_bucket'] = test_pdf['Days_Since_LastRenewalDate'].apply(lambda x: 50 if x >= 50 else (x // 10) * 10)

churn_percentage = test_pdf.groupby('Days_Since_LastRenewalDate_bucket')['churn_flag'].mean() * 100

plt.figure(figsize=(12, 8))
sns.barplot(x=churn_percentage.index, y=churn_percentage)
plt.xlabel('Days_Since_LastRenewalDate_bucket')
plt.ylabel('% Churn Flag = 1')
plt.show()

#############################

plt.figure(figsize=(12, 8))
sns.boxplot(data=test_pdf, x='churn_flag', y='LastRenewalNumber')
plt.xlabel('churn_flag')
plt.ylabel('LastRenewalNumber')
plt.show()











