{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6505b342-920d-4ceb-ae6f-58b8c29b65c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def write_read(spark, sdf, table_path):\n",
    "#     \"\"\"\n",
    "#     Writes a Spark DataFrame to a Hive table at the specified path and then reads it back.\n",
    "\n",
    "#     Parameters:\n",
    "#     - spark: The Spark Session\n",
    "#     - sdf (DataFrame): The Spark DataFrame to be written and read.\n",
    "#     - table_path (str): The Hive table path where the DataFrame is to be written.\n",
    "\n",
    "#     Returns:\n",
    "#     - DataFrame: The Spark DataFrame that has been read from the Hive table.\n",
    "#     Note:\n",
    "#     This function overwrites any existing data at the table_path.\n",
    "#     \"\"\"\n",
    "\n",
    "#     sdf.write.mode(\"overwrite\").saveAsTable(table_path)\n",
    "#     sdf = spark.read.table(table_path)\n",
    "\n",
    "#     return sdf\n",
    "\n",
    "# # def fill_with_zero(df, columns):\n",
    "# #     \"\"\"\n",
    "# #     Fill specified columns in a DataFrame with zeros.\n",
    "\n",
    "# #     Parameters:\n",
    "# #         df: DataFrame - The input DataFrame\n",
    "# #         columns: list - A list of column names to fill with zero\n",
    "\n",
    "# #     Returns:\n",
    "# #         DataFrame - The DataFrame with specified columns filled with zero\n",
    "# #     \"\"\"\n",
    "# #     fill_dict = {col: 0 for col in columns}\n",
    "# #     return df.na.fill(fill_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecf1ffcd-f097-45f2-b8a1-293707e5e40c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List to hold date type features to curate days_since features\n",
    "selected_date_columns = [\n",
    " 'AccountStatusChangedAt',\n",
    " 'AutoRenewChangeDate',\n",
    " 'GroupJoinedAt',\n",
    " 'InactivityDate',\n",
    " 'InfoRequestAt',\n",
    " 'LastRenewalDate',\n",
    " 'LostSimAt',\n",
    " #'PACRequestAt',\n",
    " #'STACRequestAt',\n",
    " 'SalesDate',\n",
    " 'LastPlanChangeAt'\n",
    "]\n",
    "\n",
    "# List to hold the features for curation of volatility features\n",
    "selected_volatility_features = [\n",
    "# Data\n",
    "\"PlanDataUKGB_LastWeek\",\n",
    "\"PlanDataUKGB_LastMonth\",\n",
    "\"PlanDataUKGB_Last6Month\",\n",
    "\n",
    "# Text\n",
    "\"PlanTextUKCount_LastWeek\",\n",
    "\"PlanTextUKCount_LastMonth\",\n",
    "\"PlanTextUKCount_Last6Month\",\n",
    "\n",
    "# Minutes\n",
    "\"PlanVoiceUKMinute_LastWeek\",\n",
    "\"PlanVoiceUKMinute_LastMonth\",\n",
    "\"PlanVoiceUKMinute_Last6Month\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f62a1f-abf0-4bae-9aaf-ac5fbaaab64c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_read(spark, sdf, table_path):\n",
    "    \"\"\"\n",
    "    Writes a Spark DataFrame to a Hive table at the specified path and then reads it back.\n",
    "\n",
    "    Parameters:\n",
    "    - spark: The Spark Session\n",
    "    - sdf (DataFrame): The Spark DataFrame to be written and read.\n",
    "    - table_path (str): The Hive table path where the DataFrame is to be written.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The Spark DataFrame that has been read from the Hive table.\n",
    "    Note:\n",
    "    This function overwrites any existing data at the table_path.\n",
    "    \"\"\"\n",
    "\n",
    "    sdf.write.mode(\"overwrite\").saveAsTable(table_path)\n",
    "    sdf = spark.read.table(table_path)\n",
    "\n",
    "    return sdf\n",
    "\n",
    "def filter_permanent_live_customers(lead_time_sdf: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the input DataFrame to only include customers who have been a customer for more than 3 months and have a live account status.\n",
    "\n",
    "    :param lead_time_sdf: Input DataFrame with customer data.\n",
    "    :return: A DataFrame with customers who have been a customer for more than 3 months and have a live account status.\n",
    "    \"\"\"\n",
    "    # Filter to include only customers who have been a customer for more than 3 months and have a live account status\n",
    "    permanent_customers_df = lead_time_sdf.filter(\n",
    "        (col(\"ActivationDate\") <= date_sub(current_date(), 90)) &\n",
    "        (col('AccountStatus') == 'Live')\n",
    "    )\n",
    "    \n",
    "    return permanent_customers_df\n",
    "\n",
    "def add_days_since_features(df: DataFrame, date_columns: list, snapshot_date_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds new columns to the DataFrame representing the number of days since the events in date_columns occurred.\n",
    "    Assumes that the snapshot date column is of date type.\n",
    "\n",
    "    :param df: Input DataFrame with date columns.\n",
    "    :param date_columns: List of column names that contain date information.\n",
    "    :param snapshot_date_col: The name of the column containing the snapshot date.\n",
    "    :return: DataFrame with new features.\n",
    "    \"\"\"\n",
    "    # Ensure the snapshot_date_col is of date type\n",
    "    df = df.withColumn(snapshot_date_col, to_date(col(snapshot_date_col)))\n",
    "\n",
    "    # Calculate the number of days since each date column\n",
    "    for date_col in date_columns:\n",
    "        days_since_col_name = f\"Days_Since_{date_col}\"\n",
    "        event_occurred_col_name = f\"Event_Occurred_{date_col}\"\n",
    "\n",
    "        # Create a binary column indicating whether the event occurred\n",
    "        df = df.withColumn(event_occurred_col_name, when(col(date_col).isNotNull(), 1).otherwise(0))\n",
    "\n",
    "        # Calculate days since the event (or set to -1 if no event occurred)\n",
    "        df = df.withColumn(days_since_col_name, when(col(event_occurred_col_name) == 1,\n",
    "                                                      datediff(col(snapshot_date_col), to_date(col(date_col))))\n",
    "                                                .otherwise(-1))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_volatility_features(df: DataFrame, selected_features: list) -> DataFrame:\n",
    "    # Define suffixes for new column names\n",
    "    abs_change_suffix = '_AbsChange'\n",
    "    pct_change_suffix = '_PctChange'\n",
    "    ratio_suffix = '_Ratio'\n",
    "    \n",
    "    # Loop through the selected features and calculate the new volatility features\n",
    "    for feature_type in ['Data', 'Text', 'Minute']:\n",
    "        # Extract relevant features for the current type (Data, Text, Minute)\n",
    "        relevant_features = [feature for feature in selected_features if feature_type in feature]\n",
    "        \n",
    "        # Ensure there are three features for each type (weekly, monthly, six-monthly)\n",
    "        if len(relevant_features) == 3:\n",
    "            weekly_feature, monthly_feature, six_monthly_feature = relevant_features\n",
    "            \n",
    "            # Calculate Absolute Changes\n",
    "            df = df.withColumn(feature_type + '_Week_Month' + abs_change_suffix, \n",
    "                               abs(col(weekly_feature) - col(monthly_feature)))\n",
    "            df = df.withColumn(feature_type + '_Month_6Month' + abs_change_suffix, \n",
    "                               abs(col(monthly_feature) - col(six_monthly_feature)))\n",
    "            \n",
    "            # Calculate Relative Changes\n",
    "            df = df.withColumn(feature_type + '_Week_Month' + pct_change_suffix, \n",
    "                               when(col(monthly_feature) != 0, \n",
    "                                    col(feature_type + '_Week_Month' + abs_change_suffix) / col(monthly_feature))\n",
    "                               .otherwise(None))\n",
    "            df = df.withColumn(feature_type + '_Month_6Month' + pct_change_suffix, \n",
    "                               when(col(six_monthly_feature) != 0, \n",
    "                                    col(feature_type + '_Month_6Month' + abs_change_suffix) / col(six_monthly_feature))\n",
    "                               .otherwise(None))\n",
    "            \n",
    "            # Calculate Ratios\n",
    "            df = df.withColumn(feature_type + '_Week_Month' + ratio_suffix, \n",
    "                               when(col(monthly_feature) != 0, \n",
    "                                    col(weekly_feature) / col(monthly_feature))\n",
    "                               .otherwise(None))\n",
    "            df = df.withColumn(feature_type + '_Month_6Month' + ratio_suffix, \n",
    "                               when(col(six_monthly_feature) != 0, \n",
    "                                    col(monthly_feature) / col(six_monthly_feature))\n",
    "                               .otherwise(None))\n",
    "            \n",
    "           \n",
    "            df = df.fillna(0)  # Replace nulls with zeros\n",
    "        else:\n",
    "            print(f\"Skipping {feature_type} as it does not have all three time frames (weekly, monthly, six-monthly).\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_features(data: DataFrame, snapshot_date_col: str):\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Processes the input DataFrame to filter ported-in live customers, add days since features,\n",
    "    and writes the result to a Hive table.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The input Spark DataFrame to be processed.\n",
    "    - snapshot_date_col: The name of the column containing the snapshot date.\n",
    "    - table_output_schema: The Hive table schema where the processed DataFrame will be written.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The Spark DataFrame that has been written to and read from the Hive table.\n",
    "    \"\"\"\n",
    "    # Filter ported-in live customers\n",
    "    filtered_data = filter_permanent_live_customers(data)\n",
    "\n",
    "    # Add days since features\n",
    "    processed_data = add_days_since_features(filtered_data, selected_date_columns, snapshot_date_col)\n",
    "\n",
    "    # Add curated volatility features from usage columns \n",
    "    processed_data = create_volatility_features(processed_data, selected_volatility_features)\n",
    "\n",
    "    # Filter processed data to customers whose days since last renewal is equal to or greater than 17\n",
    "    processed_data = processed_data.filter((col(\"Days_Since_LastRenewalDate\") >= 17))\n",
    "\n",
    "    # Write to Hive table and read back\n",
    "    #final_data = write_read(spark, processed_data, f\"{table_output_schema}.processed_data\")\n",
    "\n",
    "    return processed_data"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "featurization",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
